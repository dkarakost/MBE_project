{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMg84m4E763tbJDAHrIdOOM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dkarakost/MBE_project/blob/main/Fruitdetect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiIX5Bm-3EAZ",
        "outputId": "6a00ba0b-b1bd-4545-c36a-64839fc833dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m908.3/908.3 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.3/7.3 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m66.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hPyTorch version updated to 2.5.1.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "if torch.__version__ != '2.5.1+cu124':\n",
        "    !pip install torch==2.5.1+cu124 torchvision==0.20.1+cu124 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124 -U --quiet\n",
        "    print(\"PyTorch version updated to 2.5.1.\")\n",
        "else:\n",
        "    print(\"PyTorch is already at the correct version (2.5.1).\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install d2l==1.0.3 --quiet\n",
        "!pip install scipy --quiet\n",
        "!pip install torchmetrics --quiet"
      ],
      "metadata": {
        "id": "ZhHLBqhx4Yym",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f967001-d86f-4e8e-9e3b-10255b9d0444"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.7/111.7 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.3/98.3 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.0/125.0 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.0/95.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.3, but you have requests 2.31.0 which is incompatible.\n",
            "xarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "xarray 2025.1.2 requires pandas>=2.1, but you have pandas 2.0.3 which is incompatible.\n",
            "bigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\n",
            "chex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\n",
            "albumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "pymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\n",
            "mizani 0.13.2 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "albucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\n",
            "cvxpy 1.6.4 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "scikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\n",
            "scikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\n",
            "blosc2 3.3.0 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.23.5 which is incompatible.\n",
            "plotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\n",
            "plotnine 0.14.5 requires pandas>=2.2.0, but you have pandas 2.0.3 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\n",
            "jaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\n",
            "imbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from d2l import torch as d2l\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from PIL import ImageFile, Image\n",
        "import matplotlib.pyplot as plt\n",
        "#from object_detection_utils import ResizeWithBBox, plot_bbox, plot_grid, box_xyxy_to_cxcywh, box_xywh_to_xyxy\n",
        "import random\n",
        "import cv2\n",
        "import torchvision.models as models\n",
        "import json\n",
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "#from object_detection_utils import FileBasedAPCalculator\n",
        "from object_detection_utils import *"
      ],
      "metadata": {
        "id": "AJqJ_Khy4bs2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the dataset\n",
        "First, let's download the dataset. It consists of images of plant, bounding box annotations, and leaf counts annotations."
      ],
      "metadata": {
        "id": "TQn5QeWi4fzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://git.wur.nl/abe-datasets/education/fruit-detection-challenge.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1t0RcB_n4h-u",
        "outputId": "5ca61256-8701-4434-e626-e82ef7a89c1a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fruit-detection-challenge'...\n",
            "remote: Enumerating objects: 2010, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (6/6), done.\u001b[K\n",
            "remote: Total 2010 (delta 2), reused 0 (delta 0), pack-reused 2004 (from 1)\u001b[K\n",
            "Receiving objects: 100% (2010/2010), 255.89 MiB | 17.77 MiB/s, done.\n",
            "Resolving deltas: 100% (21/21), done.\n",
            "Updating files: 100% (1958/1958), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The COCO dataset format\n",
        "\n",
        "The labels for this detection dataset are stored using the COCO JSON format. COCO JSON is a data format used for object detection, segmentation, and keypoint annotation in images. It includes metadata like image paths, annotations, categories, and licenses, typically structured into keys such as \"images,\" \"annotations,\" \"categories,\" and more.\n",
        "\n",
        "Example structure:\n",
        "```json\n",
        "{\n",
        "  \"images\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"width\": 640,\n",
        "      \"height\": 480,\n",
        "      \"file_name\": \"image1.jpg\",\n",
        "      \"license\": 1,\n",
        "      \"date_captured\": \"2023-10-01\"\n",
        "    }\n",
        "  ],\n",
        "  \"annotations\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"image_id\": 1,\n",
        "      \"category_id\": 1,\n",
        "      \"bbox\": [50, 70, 80, 60],\n",
        "      \"area\": 4800,\n",
        "      \"segmentation\": [[121.39,215.89,...]],\n",
        "      \"iscrowd\": 0\n",
        "    }\n",
        "  ],\n",
        "  \"categories\": [\n",
        "    {\n",
        "      \"id\": 1,\n",
        "      \"name\": \"cat\",\n",
        "      \"supercategory\": \"animal\"\n",
        "    },\n",
        "    {\n",
        "      \"id\": 2,\n",
        "      \"name\": \"dog\",\n",
        "      \"supercategory\": \"animal\"\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "KPKXuNFG4rYi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# TODO: add needed imports\n",
        "\n",
        "class FruitDetectionDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, json_path, img_folder, img_size, transforms=None):\n",
        "        \"\"\"\n",
        "        Constructor of the FruitDetectionDataset\n",
        "        :param json_path: Path to the COCO JSON file\n",
        "        :param img_folder: Folder containing the images\n",
        "        :param img_size: Size to resize the images\n",
        "        :param transforms: List of transformations to be applied to the data\n",
        "        \"\"\"\n",
        "        self.img_folder = img_folder\n",
        "        self.transforms = transforms\n",
        "        self.resize = ResizeWithBBox(img_size)\n",
        "\n",
        "        with open(json_path, 'r') as f:\n",
        "            coco_data = json.load(f)\n",
        "\n",
        "        # TODO: Extract image info and annotations\n",
        "        self.images = coco_data['images']\n",
        "        self.annotations = coco_data['annotations']\n",
        "        self.categories = coco_data['categories']\n",
        "\n",
        "        # Mapping from category ID to category index\n",
        "        # This creates a zero-based index\n",
        "        self.category_id_to_index = {category['id']: idx for idx, category in enumerate(self.categories)}\n",
        "\n",
        "        # Mapping from image ID to annotations\n",
        "        self.img_id_to_annotations = {}\n",
        "        for annotation in self.annotations:\n",
        "            img_id = annotation['image_id']\n",
        "            if img_id not in self.img_id_to_annotations:\n",
        "                self.img_id_to_annotations[img_id] = []\n",
        "            self.img_id_to_annotations[img_id].append(annotation)\n",
        "\n",
        "        # Create a list of image paths\n",
        "        self.img_files = [os.path.join(img_folder, img['file_name']) for img in self.images]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # TODO Get image path and annotations\n",
        "        img_info = self.images[idx]\n",
        "        img_filename = img_info['file_name']\n",
        "        img_id = img_info['id']\n",
        "        img_path = os.path.join(self.img_folder, img_filename)\n",
        "        # Get annotations for the image\n",
        "        annotations = self.img_id_to_annotations.get(img_id, [])\n",
        "\n",
        "        # Load image\n",
        "        img = Image.open(img_path).convert(\"RGB\")  # TODO PIL Image format\n",
        "\n",
        "        # Extract bounding boxes and labels\n",
        "        bboxes = []\n",
        "        labels = []\n",
        "        for annotation in annotations:\n",
        "            x, y, width, height = annotation['bbox']\n",
        "            bboxes.append([x, y, width, height])\n",
        "            category_id = annotation['category_id']\n",
        "            labels.append(self.category_id_to_index[category_id])\n",
        "\n",
        "\n",
        "        bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        if len(bboxes.shape) == 1:\n",
        "            bboxes = bboxes.unsqueeze(0)\n",
        "\n",
        "        # TODO: Resize image and boxes\n",
        "\n",
        "        image, bboxes = self.resize(image=img, boxes=bboxes)\n",
        "\n",
        "        #convert bbox to tensor type\n",
        "        #\n",
        "        bboxes = torch.tensor(bboxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "        # TODO: Convert boxes to center x, center y, width, height format\n",
        "        cxcywh_boxes = []\n",
        "        for box in bboxes:\n",
        "          x, y, w, h = box\n",
        "          center_x = x + w / 2.0\n",
        "          center_y = y + h / 2.0\n",
        "\n",
        "          cxcywh_boxes.append([center_x, center_y, w, h])\n",
        "\n",
        "        #if bboxes is None:\n",
        "       #    bboxes = torch.zeros((0, 4), dtype=torch.float32)\n",
        "\n",
        "       # if labels is None:\n",
        "       #    labels = torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "        # Apply other transforms\n",
        "        if self.transforms:\n",
        "            image = self.transforms(image)\n",
        "\n",
        "        # TODO Add illegal boxes if needed\n",
        "        illegal_needed = max(0, 30 - len(bboxes))\n",
        "\n",
        "        illegal_labels = torch.ones((illegal_needed,), dtype=torch.int64) * -1\n",
        "\n",
        "        illegal_boxes = torch.zeros((illegal_needed, 4), dtype=torch.float32)\n",
        "        return image, {\n",
        "            \"labels\": torch.cat((labels, illegal_labels)),\n",
        "            \"boxes\": torch.cat((bboxes, illegal_boxes), axis=0),\n",
        "        }\n",
        "\n",
        "    def __len__(self): # TODO\n",
        "        return len(self.images)\n"
      ],
      "metadata": {
        "id": "zilFPNyz5JZ0"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "dataset test"
      ],
      "metadata": {
        "id": "xuqL0lb5L3-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#this is the datatset for detecter\n",
        "json_path=\"fruit-detection-challenge/detection/annotations/train.json\"\n",
        "train_folder=\"fruit-detection-challenge/detection/train/\"\n",
        "\n",
        "image_size=256\n",
        "batch_size=32\n",
        "augs = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "training_set=FruitDetectionDataset(json_path,train_folder,img_size=(image_size, image_size),transforms=augs)\n",
        "img,target=training_set[0]\n",
        "\n",
        "# print(\"Type of img:\", type(img))\n",
        "# print(\"Type of target:\", type(target))\n",
        "display_imgs_bbox = []\n",
        "# print(\"output of the dataset:\",target)\n",
        "max_boxes = max(len(training_set.img_id_to_annotations[img['id']]) for img in training_set.images)\n",
        "# print(\"Maximum boxes in one image:\", max_boxes)\n",
        "for i in range(10):\n",
        "    img, target =training_set[i]\n",
        "\n",
        "    labels = target[\"labels\"]\n",
        "    boxes = target[\"boxes\"]\n",
        "    img = plot_bbox(img, boxes[labels != -1] * image_size, labels[labels != -1].numpy())\n",
        "    display_imgs_bbox.append(img)\n",
        "\n",
        "# Plot two grids, one per list (don't forget the functions declared in the beginning of this notebook)\n",
        "plot_grid(imgs=display_imgs_bbox, nrows=2, ncols=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "9Esl8l2JL3aw",
        "outputId": "a055da4e-84d6-4c47-9ef1-0d2c9882c026"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-36-a5163fa3c51d>:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = torch.tensor(labels, dtype=torch.int64)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'Tensor' object has no attribute 'load'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageDraw.py\u001b[0m in \u001b[0;36mDraw\u001b[0;34m(im, mode)\u001b[0m\n\u001b[1;32m    992\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"getdraw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'getdraw'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-327cb6908c68>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"labels\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"boxes\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplot_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimage_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mdisplay_imgs_bbox\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/object_detection_utils.py\u001b[0m in \u001b[0;36mplot_bbox\u001b[0;34m(img, boxes, labels)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImage\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mrepresenting\u001b[0m \u001b[0mthe\u001b[0m \u001b[0moriginal\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbounding\u001b[0m \u001b[0mboxes\u001b[0m \u001b[0mplotted\u001b[0m \u001b[0mon\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \"\"\"\n\u001b[0;32m--> 279\u001b[0;31m     \u001b[0mdraw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m     \u001b[0mcolors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"red\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"blue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"green\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"yellow\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"purple\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"orange\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pink\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"cyan\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"magenta\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lime\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageDraw.py\u001b[0m in \u001b[0;36mDraw\u001b[0;34m(im, mode)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"getdraw\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 995\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    996\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/PIL/ImageDraw.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, im, mode)\u001b[0m\n\u001b[1;32m     78\u001b[0m            \u001b[0mdefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \"\"\"\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadonly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# make it writeable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'load'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_dir = \"fruit-detection-challenge/classification\"\n",
        "\n",
        "train_imgs = torchvision.datasets.ImageFolder(\n",
        "    os.path.join(data_dir, 'train'))\n",
        "val_imgs = torchvision.datasets.ImageFolder(\n",
        "    os.path.join(data_dir, 'val'))"
      ],
      "metadata": {
        "id": "30DDSist5k18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_imgs.classes"
      ],
      "metadata": {
        "id": "BUqF6qtF5mlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class_names = train_imgs.classes\n",
        "images = []\n",
        "\n",
        "for i in range(len(train_imgs.classes)):\n",
        "    class_idx = i % len(class_names)\n",
        "    class_name = class_names[class_idx]\n",
        "    class_images = [img for img, label in train_imgs if label == class_idx]\n",
        "    for i in range(4): # Select 4 random images from the class\n",
        "        images.append(random.choice(class_images))\n",
        "\n",
        "d2l.show_images(images, 2, 8, scale=1.4)\n"
      ],
      "metadata": {
        "id": "z_aqAycQ5p9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Detecter"
      ],
      "metadata": {
        "id": "g6UdbIO_Bzls"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQNHLZ0SlhRp"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class PredictionHead(nn.Module):\n",
        "    def __init__(self, in_channels, num_classes):\n",
        "        super(PredictionHead, self).__init__()\n",
        "\n",
        "        self.conv_layers = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, in_channels // 2, kernel_size=3, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(in_channels // 2),\n",
        "            nn.LeakyReLU(negative_slope=0.1),\n",
        "            nn.Conv2d(in_channels // 2, in_channels // 4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_channels // 4),\n",
        "            nn.LeakyReLU(negative_slope=0.1),\n",
        "            nn.Conv2d(in_channels // 4, in_channels // 4, kernel_size=1, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(in_channels // 4),\n",
        "            nn.LeakyReLU(negative_slope=0.1),\n",
        "        )\n",
        "\n",
        "        self.box_predictor = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels // 4, out_channels=in_channels // 4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_channels // 4),\n",
        "            nn.LeakyReLU(negative_slope=0.1),\n",
        "            nn.Conv2d(in_channels=in_channels // 4, out_channels=4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels // 4, out_channels=in_channels // 4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_channels // 4),\n",
        "            nn.LeakyReLU(negative_slope=0.1),\n",
        "            nn.Conv2d(in_channels=in_channels // 4, out_channels=num_classes, kernel_size=3, stride=1, padding=1),\n",
        "        )\n",
        "\n",
        "        # Objectness score predictor\n",
        "        self.objectness_predictor = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=in_channels // 4, out_channels=in_channels // 4, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(in_channels // 4),\n",
        "            nn.LeakyReLU(negative_slope=0.1),\n",
        "            nn.Conv2d(in_channels=in_channels // 4, out_channels=1, kernel_size=3, stride=1, padding=1),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "    '''\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        cls_logits = self.classifier(x)\n",
        "        bbox_pred = self.box_predictor(x)\n",
        "        objectness_pred = self.objectness_predictor(x)\n",
        "\n",
        "        # Add relative grid position to the cx and cy predictions of each box\n",
        "        grid_size = x.shape[-1]\n",
        "        grid_y, grid_x = torch.meshgrid(\n",
        "            torch.arange(grid_size, device=x.device), torch.arange(grid_size, device=x.device), indexing=\"ij\"\n",
        "        )\n",
        "        cx = bbox_pred[:, 0, :, :]\n",
        "        cy = bbox_pred[:, 1, :, :]\n",
        "        w = bbox_pred[:, 2, :, :]\n",
        "        h = bbox_pred[:, 3, :, :]\n",
        "        cx = (cx + (grid_x)) / grid_size\n",
        "        cy = (cy + (grid_y)) / grid_size\n",
        "        bbox_pred = torch.stack([cx, cy, w, h], dim=1)\n",
        "'''\n",
        "    def forward(self, x):\n",
        "        x = self.conv_layers(x)\n",
        "        cls_logits = self.classifier(x)\n",
        "        bbox_pred = self.box_predictor(x)\n",
        "        objectness_pred = self.objectness_predictor(x)\n",
        "        grid_h, grid_w = bbox_pred.shape[2], bbox_pred.shape[3]\n",
        "        grid_y, grid_x = torch.meshgrid(\n",
        "        torch.arange(grid_h, device=bbox_pred.device),\n",
        "        torch.arange(grid_w, device=bbox_pred.device),\n",
        "        indexing=\"ij\"\n",
        "         )\n",
        "        cx = bbox_pred[:, 0, :, :]\n",
        "        cy = bbox_pred[:, 1, :, :]\n",
        "        w = bbox_pred[:, 2, :, :]\n",
        "        h = bbox_pred[:, 3, :, :]\n",
        "        cx = (cx + grid_x) / grid_w\n",
        "        cy = (cy + grid_y) / grid_h\n",
        "        bbox_pred = torch.stack([cx, cy, w, h], dim=1)\n",
        "        return cls_logits, bbox_pred, objectness_pred\n",
        "\n",
        "\n",
        "\n",
        "class ObjectDetector(nn.Module):\n",
        "    def __init__(self, n_classes, pretrained=True):\n",
        "        super(ObjectDetector, self).__init__()\n",
        "\n",
        "        # We add the background class\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Backbone\n",
        "        backbone = models.resnet18(pretrained=pretrained)\n",
        "        self.backbone = nn.Sequential(*list(backbone.children())[:-3])\n",
        "\n",
        "        # Layer 1\n",
        "        self.block_1 = nn.Sequential(*list(backbone.children())[-3])\n",
        "\n",
        "        self.prediction_head = PredictionHead(512, self.n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        y_ = self.block_1(features)\n",
        "\n",
        "        cls_logits, bbox_pred, objectness_pred = self.prediction_head(y_)\n",
        "        cls_preds = cls_logits.flatten(2, 3).permute(0, 2, 1)\n",
        "        box_preds = bbox_pred.flatten(2, 3).permute(0, 2, 1)\n",
        "        obj_preds = objectness_pred.flatten(2, 3).permute(0, 2, 1)\n",
        "\n",
        "        return {\"pred_logits\": cls_preds, \"pred_boxes\": box_preds, \"pred_objectness\": obj_preds}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "add noise to validation set"
      ],
      "metadata": {
        "id": "DBNr_6pfCAJ9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_noise_to_box(box_coords, noise_factor=0.1):\n",
        "    noisy_box = []\n",
        "    for coord in box_coords:\n",
        "        noisy_coord = np.random.normal(loc=coord, scale=noise_factor * coord)\n",
        "        noisy_box.append(max(0, min(noisy_coord, 1)))  # Ensure the values are within [0, 1]\n",
        "    return noisy_box\n",
        "\n",
        "def remove_random_predictions(predictions, removal_fraction=0.2):\n",
        "    num_predictions = len(predictions)\n",
        "    indices_to_remove = np.random.choice(num_predictions, size=int(removal_fraction * num_predictions), replace=False)\n",
        "    remaining_predictions = [pred for i, pred in enumerate(predictions) if i not in indices_to_remove]\n",
        "    return remaining_predictions\n",
        "\n",
        "def process_predictions(file_path):\n",
        "    predictions = []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split(',')\n",
        "            image_name = parts[0]\n",
        "            box_coords = list(map(float, parts[1:5]))\n",
        "            class_id = int(parts[5])\n",
        "            confidence = 1.0\n",
        "            predictions.append((image_name, *box_coords, class_id, confidence))\n",
        "\n",
        "    noisy_predictions = []\n",
        "    for pred in predictions:\n",
        "        image_name, x_center, y_center, width, height, class_id, conf = pred\n",
        "        noisy_box = add_noise_to_box([x_center, y_center, width, height], 0.05)\n",
        "        noisy_predictions.append((image_name, *noisy_box, class_id, conf))\n",
        "\n",
        "    remaining_predictions = remove_random_predictions(noisy_predictions, 0.1)\n",
        "\n",
        "    return remaining_predictions\n",
        "\n",
        "def save_processed_predictions(predictions, output_file_path):\n",
        "    with open(output_file_path, 'w') as f:\n",
        "        for pred in predictions:\n",
        "            line = ','.join(map(str, pred))\n",
        "            f.write(line + '\\n')\n"
      ],
      "metadata": {
        "id": "7Z7GzyfWBxcu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "val_gt = \"fruit-detection-challenge/detection/annotations/val.txt\"\n",
        "\n",
        "noisy_val_gt = 'processed_predictions.txt'\n",
        "\n",
        "predictions = process_predictions(val_gt)\n",
        "save_processed_predictions(predictions, noisy_val_gt)\n",
        "\n",
        "#AP calculater used to evaluate model\n",
        "ap_calculator = FileBasedAPCalculator(\n",
        "    val_gt,\n",
        "    noisy_val_gt,\n",
        ")\n",
        "ap = ap_calculator.calculate_map()"
      ],
      "metadata": {
        "id": "RxmmuuYrCVrx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build training set and validation set"
      ],
      "metadata": {
        "id": "QqJAaNMNA4CB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "json_path=\"fruit-detection-challenge/detection/annotations/train.json\"\n",
        "train_folder=\"fruit-detection-challenge/detection/train/\"\n",
        "val_gt = \"fruit-detection-challenge/detection/annotations/val.txt\"\n",
        "image_size=256\n",
        "batch_size=32\n",
        "#transform\n",
        "augs = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "#trainig set\n",
        "training_set=FruitDetectionDataset(json_path,train_folder,img_size=(image_size, image_size),transforms=augs)\n",
        "#Training set loader\n",
        "train_loader = torch.utils.data.DataLoader(training_set, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "#validation set\n",
        "val_set=FruitDetectionDataset(val_gt,train_folder,img_size=(image_size, image_size),transforms=augs)\n",
        "val_loader=torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=2)"
      ],
      "metadata": {
        "id": "HgcFSoyJ-b3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multiscale"
      ],
      "metadata": {
        "id": "ouM9X_9P835F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ObjectDetectorMultiScale(nn.Module):\n",
        "    def __init__(self, n_classes, pretrained=True):\n",
        "        super(ObjectDetectorMultiScale, self).__init__()\n",
        "\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "        # Backbone\n",
        "        backbone = models.resnet18(pretrained=pretrained)\n",
        "        self.backbone = nn.Sequential(*list(backbone.children())[:-3])\n",
        "\n",
        "        # Layer 1\n",
        "        self.block_1 = nn.Sequential(*list(backbone.children())[-3])\n",
        "\n",
        "        self.prediction_head = PredictionHead(512, self.n_classes)\n",
        "        self.prediction_head_2 = PredictionHead(256, self.n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x)\n",
        "        y_ = self.block_1(features)\n",
        "\n",
        "        cls_logits, bbox_pred, objectness_pred = self.prediction_head(y_)\n",
        "        cls_preds = cls_logits.flatten(2, 3).permute(0, 2, 1)\n",
        "        box_preds = bbox_pred.flatten(2, 3).permute(0, 2, 1)\n",
        "        obj_preds = objectness_pred.flatten(2, 3).permute(0, 2, 1)\n",
        "\n",
        "        cls_logits2, bbox_pred2, objectness_pred2 = self.prediction_head_2(features)\n",
        "        cls_preds2 = cls_logits2.flatten(2, 3).permute(0, 2, 1)\n",
        "        box_preds2 = bbox_pred2.flatten(2, 3).permute(0, 2, 1)\n",
        "        obj_preds2 = objectness_pred2.flatten(2, 3).permute(0, 2, 1)\n",
        "\n",
        "        # Concatenate predictions from all heads\n",
        "        pred_logits = torch.cat([cls_preds, cls_preds2], dim=1)\n",
        "        pred_boxes = torch.cat([box_preds, box_preds2], dim=1)\n",
        "        pred_objectness = torch.cat([obj_preds, obj_preds2], dim=1)\n",
        "\n",
        "        return {\"pred_logits\": pred_logits, \"pred_boxes\": pred_boxes, \"pred_objectness\": pred_objectness}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "gJnMfbDR82ws",
        "outputId": "2d0c726a-67c7-452b-fbc4-90784517f34d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-6ecc5cf59c74>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mObjectDetectorMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpretrained\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mObjectDetectorMultiScale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "9ajYfSCWlhRs",
        "outputId": "e28277c5-b904-41ec-9767-218e23168bd0"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ObjectDetectorMultiScale' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-8bc8534ac994>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mObjectDetectorMultiScale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'ObjectDetectorMultiScale' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "def train_model(model, train_loader, val_loader, num_epochs=50, lr=1e-3, weight_decay=1e-4, step_size=20, gamma=0.1):\n",
        "    device = d2l.try_gpu()\n",
        "    model = model.to(device)\n",
        "    matcher = HungarianMatcher()\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
        "    print(\"total num of parameters in the model:\", sum(p.numel() for p in model.parameters()))\n",
        "\n",
        "    timer = d2l.Timer()\n",
        "    animator = d2l.Animator(xlabel=\"epoch\", xlim=[1, num_epochs], legend=[\"class error\", \"L1 error\", \"val AP\"])\n",
        "\n",
        "    ap_calculator = APCalculator(val_loader)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        metric = d2l.Accumulator(4)\n",
        "        model.train()\n",
        "        for img, targets in train_loader:\n",
        "            timer.start()\n",
        "\n",
        "            img = img.to(device)\n",
        "\n",
        "            # Remove illegal targets\n",
        "            new_targets = []\n",
        "            for i in range(targets[\"labels\"].shape[0]):\n",
        "                labels = targets[\"labels\"][i]\n",
        "                boxes = targets[\"boxes\"][i]\n",
        "                new_targets.append({\"labels\": labels[labels != -1].to(device), \"boxes\": boxes[labels != -1].to(device)})\n",
        "\n",
        "            outputs = model(img)\n",
        "\n",
        "            num_boxes = sum(len(t[\"labels\"]) for t in new_targets)\n",
        "            num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "\n",
        "            src_logits = outputs[\"pred_logits\"]\n",
        "            src_boxes = outputs[\"pred_boxes\"]\n",
        "            src_objectness = outputs[\"pred_objectness\"]\n",
        "\n",
        "            indices = matcher(outputs, new_targets)  # Run matcher\n",
        "            idx = get_src_permutation_idx(indices)\n",
        "\n",
        "            # Loss class\n",
        "            target_classes_o = torch.cat([t[\"labels\"][J] for t, (_, J) in zip(new_targets, indices)])\n",
        "            target_classes = torch.full(src_logits.shape[:2], -100, dtype=torch.int64, device=device)\n",
        "            target_classes[idx] = target_classes_o\n",
        "            loss_ce = F.cross_entropy(src_logits.transpose(1, 2), target_classes, ignore_index=-100)\n",
        "\n",
        "            # Loss boxes L1 and GIOU\n",
        "            src_boxes = src_boxes[idx]\n",
        "            target_boxes = torch.cat([t[\"boxes\"][i] for t, (_, i) in zip(new_targets, indices)], dim=0)\n",
        "\n",
        "            loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction=\"none\")\n",
        "            loss_bbox = loss_bbox.sum() / num_boxes\n",
        "\n",
        "            # Loss objectness\n",
        "            target_objectness = torch.zeros_like(src_objectness, device=device)\n",
        "            target_objectness[idx] = 1\n",
        "            loss_objectness = F.mse_loss(src_objectness, target_objectness, reduction=\"mean\")\n",
        "\n",
        "            # Sum the losses\n",
        "            loss = loss_ce + loss_bbox + loss_objectness\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            metric.add(\n",
        "                loss_ce.cpu().detach().numpy(),\n",
        "                loss_bbox.cpu().detach().numpy(),\n",
        "                loss_objectness.cpu().detach().numpy(),\n",
        "                img.size()[0],\n",
        "            )\n",
        "        scheduler.step()\n",
        "        cls_err, L1_error, obj_error = metric[0] / metric[3], metric[1] / metric[3], metric[2] / metric[3]\n",
        "        ap = ap_calculator.calculate_map(model, nms_threshold=0.5)\n",
        "        print(ap)\n",
        "        animator.add(epoch + 1, (cls_err, L1_error, ap[\"map_50\"]))\n",
        "    print(f\"{len(train_loader.dataset) / timer.stop():.1f} examples/sec on {str(device)}\")\n",
        "\n",
        "\n",
        "model = ObjectDetectorMultiScale(n_classes=4)\n",
        "train_model(model, train_loader, val_loader, num_epochs=20, lr=1e-3, weight_decay=1e-4, step_size=10, gamma=0.1)"
      ]
    }
  ]
}